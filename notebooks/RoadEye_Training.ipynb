{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoadEye Wildlife Detection - YOLO11 Training with NAM Attention\n",
    "\n",
    "This notebook trains a YOLO11 model with NAM (Normalisation-based Attention Module) for detecting Tasmanian wildlife roadkill.\n",
    "\n",
    "**Features:**\n",
    "- **YOLO11** - Latest Ultralytics model with 22% fewer parameters than YOLOv8\n",
    "- **NAM Attention** - Normalization-based attention for better feature extraction\n",
    "- **FiftyOne Integration** - Visualise and analyse your dataset\n",
    "- Transfer learning with frozen backbone to preserve pre-trained animal features\n",
    "- Exports trained model for deployment\n",
    "\n",
    "**Target Species:**\n",
    "- Tasmanian Devil (endangered)\n",
    "- Feral Cat\n",
    "- Tasmanian Pademelon\n",
    "- Bennett's Wallaby\n",
    "- Bare-nosed Wombat\n",
    "- Brushtail Possum\n",
    "- Fallow Deer\n",
    "- Southern Brown Bandicoot\n",
    "- Currawong\n",
    "- Bronzewing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU availability\n!nvidia-smi\n\n# Install dependencies\n# Pin requests-ratelimiter to avoid BucketFullException import error in pyinaturalist\n!pip install -q ultralytics>=8.3.0 \"requests-ratelimiter<0.8\" pyinaturalist tqdm pyyaml\n\nimport os\n\n# Try to mount Google Drive (optional - skip if it fails)\nDRIVE_OUTPUT = None\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DRIVE_OUTPUT = '/content/drive/MyDrive/RoadEye'\n    os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n    os.makedirs(f'{DRIVE_OUTPUT}/models', exist_ok=True)\n    print(f\"Drive mounted. Output directory: {DRIVE_OUTPUT}\")\nexcept Exception as e:\n    print(f\"Drive mount skipped ({e})\")\n    print(\"Models will be saved locally at /content/roadeye_output/\")\n    DRIVE_OUTPUT = '/content/roadeye_output'\n    os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n    os.makedirs(f'{DRIVE_OUTPUT}/models', exist_ok=True)\n\n# Check ultralytics version\nimport ultralytics\nprint(f\"Ultralytics version: {ultralytics.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "CONFIG = {\n",
    "    # Project settings\n",
    "    \"project_name\": \"roadeye-wildlife-yolo11\",\n",
    "    \n",
    "    # Base model options (YOLO11 recommended):\n",
    "    # - \"yolo11n.pt\" (nano, fastest, 2.6M params)\n",
    "    # - \"yolo11s.pt\" (small, 9.4M params)\n",
    "    # - \"yolo11m.pt\" (medium, recommended, 20.1M params)\n",
    "    # - \"yolo11l.pt\" (large, 25.3M params)\n",
    "    # - \"yolo11x.pt\" (extra large, most accurate, 56.9M params)\n",
    "    \"base_model\": \"yolo11m.pt\",\n",
    "    \n",
    "    # NAM Attention settings\n",
    "    \"use_nam_attention\": True,\n",
    "    \n",
    "    # Training parameters\n",
    "    \"epochs_phase1\": 50,      # Frozen backbone training\n",
    "    \"epochs_phase2\": 50,      # Fine-tuning (optional)\n",
    "    \"batch_size\": 16,\n",
    "    \"image_size\": 640,\n",
    "    \"patience\": 20,           # Early stopping patience\n",
    "    \"freeze_layers\": 10,      # Layers to freeze (backbone)\n",
    "    \n",
    "    # Species to train (MEWC class ID -> species info)\n",
    "    # Taxon IDs verified against iNaturalist API\n",
    "    \"species\": {\n",
    "        0: {\"code\": \"DEVIL\", \"scientific\": \"Sarcophilus harrisii\", \"taxon_id\": 40232},\n",
    "        1: {\"code\": \"FCAT\", \"scientific\": \"Felis catus\", \"taxon_id\": 118552},\n",
    "        2: {\"code\": \"PADEM\", \"scientific\": \"Thylogale billardierii\", \"taxon_id\": 42970},\n",
    "        3: {\"code\": \"WALBY\", \"scientific\": \"Notamacropus rufogriseus\", \"taxon_id\": 1453431},\n",
    "        4: {\"code\": \"WOMBAT\", \"scientific\": \"Vombatus ursinus\", \"taxon_id\": 43009},\n",
    "        5: {\"code\": \"BPOSM\", \"scientific\": \"Trichosurus vulpecula\", \"taxon_id\": 42808},\n",
    "        6: {\"code\": \"FDEER\", \"scientific\": \"Dama dama\", \"taxon_id\": 42161},\n",
    "        7: {\"code\": \"BANDI\", \"scientific\": \"Isoodon obesulus\", \"taxon_id\": 43294},\n",
    "        8: {\"code\": \"CURRA\", \"scientific\": \"Strepera graculina\", \"taxon_id\": 8423},\n",
    "        9: {\"code\": \"BRONZ\", \"scientific\": \"Phaps chalcoptera\", \"taxon_id\": 3335},\n",
    "    },\n",
    "    \n",
    "    # Data collection settings\n",
    "    \"max_images_per_species\": 500,  # Increased for better training\n",
    "    \"include_dead_only\": False,\n",
    "    \"train_val_split\": 0.8,\n",
    "}\n",
    "\n",
    "# Display config\n",
    "print(\"Training Configuration\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Base model: {CONFIG['base_model']} (YOLO11)\")\n",
    "print(f\"NAM Attention: {CONFIG['use_nam_attention']}\")\n",
    "print(f\"Epochs (Phase 1 - frozen): {CONFIG['epochs_phase1']}\")\n",
    "print(f\"Epochs (Phase 2 - fine-tune): {CONFIG['epochs_phase2']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Image size: {CONFIG['image_size']}\")\n",
    "print(f\"Species count: {len(CONFIG['species'])}\")\n",
    "print(f\"Max images per species: {CONFIG['max_images_per_species']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NAM Attention Module\n",
    "\n",
    "The Normalisation-based Attention Module (NAM) uses batch normalisation scaling factors to measure channel importance. This is more efficient than CBAM while maintaining competitive performance.\n",
    "\n",
    "Reference: [NAM: Normalization-based Attention Module](https://arxiv.org/abs/2111.12419)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\n\n\nclass NAMChannelAttention(nn.Module):\n    \"\"\"NAM Channel Attention Module.\n    \n    Uses batch normalisation scaling factor to represent channel importance.\n    More efficient than SE/CBAM channel attention.\n    \"\"\"\n    \n    def __init__(self, channels: int, reduction: int = 4):\n        super().__init__()\n        self.channels = channels\n        self.bn = nn.BatchNorm2d(channels, affine=True)\n        self.gamma = nn.Parameter(torch.zeros(1, channels, 1, 1))\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        bn_weight = self.bn.weight.data.abs()\n        bn_weight = bn_weight / (bn_weight.sum() + 1e-8)\n        out = self.bn(x)\n        weight = self.gamma * bn_weight.view(1, -1, 1, 1)\n        attention = self.sigmoid(weight)\n        return out * attention\n\n\nclass NAMSpatialAttention(nn.Module):\n    \"\"\"NAM Spatial Attention Module.\"\"\"\n    \n    def __init__(self, channels: int):\n        super().__init__()\n        self.bn = nn.BatchNorm2d(channels, affine=True)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        out = self.bn(x)\n        attention = self.sigmoid(out)\n        return x * attention\n\n\nclass NAMBlock(nn.Module):\n    \"\"\"Combined NAM Attention Block (Channel + Spatial) with residual connection.\n    \n    Drop-in module for YOLO architectures.\n    \"\"\"\n    \n    def __init__(self, channels: int, use_spatial: bool = True):\n        super().__init__()\n        self.channel_attention = NAMChannelAttention(channels)\n        self.use_spatial = use_spatial\n        if use_spatial:\n            self.spatial_attention = NAMSpatialAttention(channels)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        out = self.channel_attention(x)\n        if self.use_spatial:\n            out = self.spatial_attention(out)\n        return out + x  # Residual connection\n\n\n# Register NAMBlock with Ultralytics so the custom YAML can reference it\nif CONFIG[\"use_nam_attention\"]:\n    try:\n        import ultralytics.nn.modules as modules\n        from ultralytics.nn import tasks\n        \n        if not hasattr(modules, \"NAMBlock\"):\n            modules.NAMBlock = NAMBlock\n        if not hasattr(tasks, \"NAMBlock\"):\n            tasks.NAMBlock = NAMBlock\n        \n        print(\"NAMBlock registered with Ultralytics\")\n        print(\"  Custom YOLO11 architecture will use NAM attention blocks\")\n        print(\"  Architecture YAML: configs/yolo11m-nam.yaml\")\n    except ImportError:\n        print(\"Ultralytics not yet imported - NAM will be registered before training\")\nelse:\n    print(\"NAM Attention disabled in config\")\n\nprint(\"\\nNAM Attention modules defined:\")\nprint(\"  - NAMChannelAttention: BN scaling for channel importance\")\nprint(\"  - NAMSpatialAttention: Pixel-wise attention via BN\")\nprint(\"  - NAMBlock: Combined with residual connection\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Load Training Data\n\nDownloads images from iNaturalist for each species. This takes ~20-30 minutes on first run.\n\nIf you already have `roadeye_training_images.zip` on Google Drive, set `LOAD_FROM_DRIVE = True` below to skip the download."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import zipfile\nimport shutil\nimport time\nimport requests\nfrom pathlib import Path\nfrom typing import List, Dict, Optional\nfrom dataclasses import dataclass, field\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\n\n# === Choose data loading method ===\nLOAD_FROM_DRIVE = False  # Set to True if you uploaded roadeye_training_images.zip to Drive\n\n# Where the zip lives on Google Drive (only used if LOAD_FROM_DRIVE=True)\nDRIVE_ZIP = f\"{DRIVE_OUTPUT}/roadeye_training_images.zip\"\n# Local working directory on Colab\nRAW_DIR = Path(\"/content/data/training/raw\")\n\n@dataclass\nclass ObservationRecord:\n    \"\"\"Record of a downloaded observation.\"\"\"\n    observation_id: str\n    species_code: str\n    class_id: int\n    latitude: Optional[float] = None\n    longitude: Optional[float] = None\n    observed_on: Optional[str] = None\n    image_url: str = \"\"\n    local_path: Optional[str] = None\n    is_dead: bool = False\n    source: str = \"unknown\"\n\nif LOAD_FROM_DRIVE:\n    # ---- Load from Google Drive zip ----\n    print(\"Loading training data from Google Drive\")\n    print(\"=\" * 50)\n\n    if not Path(DRIVE_ZIP).exists():\n        raise FileNotFoundError(\n            f\"Zip not found at {DRIVE_ZIP}\\n\"\n            f\"Upload roadeye_training_images.zip to Google Drive under MyDrive/RoadEye/\"\n        )\n\n    print(f\"Unzipping {DRIVE_ZIP} to /content/ ...\")\n    with zipfile.ZipFile(DRIVE_ZIP, \"r\") as zf:\n        zf.extractall(\"/content/\")\n    print(\"Unzipped.\")\n\n    configs_dir = Path(\"/content/configs\")\n    if configs_dir.exists():\n        print(f\"  Found configs/ directory with NAM YAML\")\n\n    all_observations = []\n    class_names = [CONFIG[\"species\"][i][\"code\"] for i in range(len(CONFIG[\"species\"]))]\n    class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n    extra_classes = [\"FOX\", \"HARE\", \"HEDGE\", \"OTHER\"]\n    for i, ec in enumerate(extra_classes):\n        if ec not in class_to_idx:\n            class_to_idx[ec] = len(class_names) + i\n\n    for source_dir in sorted(RAW_DIR.iterdir()):\n        if not source_dir.is_dir():\n            continue\n        source_name = source_dir.name\n        for species_dir in sorted(source_dir.iterdir()):\n            if not species_dir.is_dir():\n                continue\n            species_code = species_dir.name\n            cid = class_to_idx.get(species_code, -1)\n            if cid == -1:\n                continue\n            for img_path in sorted(species_dir.iterdir()):\n                if img_path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".webp\"):\n                    continue\n                all_observations.append(ObservationRecord(\n                    observation_id=img_path.stem,\n                    species_code=species_code,\n                    class_id=cid,\n                    local_path=str(img_path),\n                    source=source_name,\n                ))\n\n    print(f\"\\nLoaded {len(all_observations)} images from Drive\")\n\nelse:\n    # ---- Download fresh from iNaturalist ----\n    print(\"Downloading training data from iNaturalist\")\n    print(\"=\" * 50)\n    print(\"This takes ~20-30 minutes. Set LOAD_FROM_DRIVE=True to skip next time.\\n\")\n\n    from pyinaturalist import get_observations\n\n    class INaturalistCollector:\n        def __init__(self, output_dir: str = \"/content/data/training/raw/inaturalist\"):\n            self.output_dir = Path(output_dir)\n            self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        def collect_species(self, class_id, species_code, taxon_id, max_images=500, dead_only=False):\n            params = {\n                \"taxon_id\": taxon_id, \"quality_grade\": \"research\",\n                \"photos\": True, \"per_page\": min(200, max_images),\n            }\n            if dead_only:\n                params[\"term_id\"] = 17\n                params[\"term_value_id\"] = 19\n\n            print(f\"  Fetching {species_code} from iNaturalist...\")\n            all_results = []\n            page = 1\n            while len(all_results) < max_images:\n                try:\n                    params[\"page\"] = page\n                    response = get_observations(**params)\n                    results = response.get(\"results\", [])\n                    if not results:\n                        break\n                    all_results.extend(results)\n                    page += 1\n                    time.sleep(1)\n                except Exception as e:\n                    print(f\"  API error: {e}\")\n                    break\n\n            species_dir = self.output_dir / species_code\n            species_dir.mkdir(parents=True, exist_ok=True)\n\n            observations = []\n            for obs in tqdm(all_results[:max_images], desc=f\"  {species_code}\", leave=False):\n                if not obs.get(\"photos\"):\n                    continue\n                location = obs.get(\"location\")\n                lat, lng = None, None\n                if location:\n                    try:\n                        lat, lng = map(float, location.split(\",\"))\n                    except:\n                        pass\n                photo = obs[\"photos\"][0]\n                image_url = photo.get(\"url\", \"\").replace(\"square\", \"medium\")\n                is_dead = any(\n                    a.get(\"controlled_attribute\", {}).get(\"id\") == 17\n                    and a.get(\"controlled_value\", {}).get(\"id\") == 19\n                    for a in obs.get(\"annotations\", [])\n                )\n                filename = f\"{obs['id']}.jpg\"\n                filepath = species_dir / filename\n                if not filepath.exists():\n                    try:\n                        r = requests.get(image_url, timeout=30)\n                        if r.status_code == 200:\n                            filepath.write_bytes(r.content)\n                        time.sleep(1.0)\n                    except:\n                        continue\n                if filepath.exists():\n                    observations.append(ObservationRecord(\n                        observation_id=str(obs[\"id\"]), species_code=species_code,\n                        class_id=class_id, latitude=lat, longitude=lng,\n                        observed_on=obs.get(\"observed_on\"), image_url=image_url,\n                        local_path=str(filepath), is_dead=is_dead, source=\"inaturalist\",\n                    ))\n            print(f\"  Downloaded {len(observations)} images for {species_code}\")\n            return observations\n\n    collector = INaturalistCollector()\n    all_observations = []\n    for class_id, species_info in CONFIG[\"species\"].items():\n        print(f\"\\n[{class_id + 1}/{len(CONFIG['species'])}] {species_info['code']}\")\n        observations = collector.collect_species(\n            class_id=class_id, species_code=species_info[\"code\"],\n            taxon_id=species_info[\"taxon_id\"],\n            max_images=CONFIG[\"max_images_per_species\"],\n            dead_only=CONFIG[\"include_dead_only\"],\n        )\n        all_observations.extend(observations)\n\n# Summary\nprint(f\"\\n{'=' * 50}\")\nprint(f\"Total images: {len(all_observations)}\")\nspecies_counts = Counter(o.species_code for o in all_observations)\nsource_counts = Counter(o.source for o in all_observations)\nprint(f\"\\nBy source:\")\nfor src, count in sorted(source_counts.items()):\n    print(f\"  {src}: {count}\")\nprint(f\"\\nBy species:\")\nfor species, count in sorted(species_counts.items()):\n    print(f\"  {species}: {count}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create YOLO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "def create_yolo_dataset(\n",
    "    observations: List[ObservationRecord],\n",
    "    output_dir: str = \"/content/dataset_yolo\",\n",
    "    train_ratio: float = 0.8,\n",
    ") -> str:\n",
    "    \"\"\"Create YOLO-format dataset from observations.\n",
    "    \n",
    "    Returns path to data.yaml file.\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Create directory structure\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        (output_path / split / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "        (output_path / split / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    # Get class names in order\n",
    "    class_names = [CONFIG[\"species\"][i][\"code\"] for i in range(len(CONFIG[\"species\"]))]\n",
    "    \n",
    "    # Filter valid observations\n",
    "    valid_obs = [o for o in observations if o.local_path and Path(o.local_path).exists()]\n",
    "    print(f\"Valid observations with images: {len(valid_obs)}\")\n",
    "    \n",
    "    # Shuffle and split\n",
    "    random.shuffle(valid_obs)\n",
    "    split_idx = int(len(valid_obs) * train_ratio)\n",
    "    train_obs = valid_obs[:split_idx]\n",
    "    val_obs = valid_obs[split_idx:]\n",
    "    \n",
    "    print(f\"Train set: {len(train_obs)}\")\n",
    "    print(f\"Val set: {len(val_obs)}\")\n",
    "    \n",
    "    def process_split(obs_list, split_name):\n",
    "        for obs in tqdm(obs_list, desc=f\"Processing {split_name}\"):\n",
    "            src_path = Path(obs.local_path)\n",
    "            \n",
    "            # Copy image\n",
    "            dst_img = output_path / split_name / \"images\" / src_path.name\n",
    "            shutil.copy(src_path, dst_img)\n",
    "            \n",
    "            # Create label (full image bounding box as placeholder)\n",
    "            # Format: class_id x_center y_center width height (normalised)\n",
    "            label_content = f\"{obs.class_id} 0.5 0.5 1.0 1.0\\n\"\n",
    "            \n",
    "            label_path = output_path / split_name / \"labels\" / f\"{src_path.stem}.txt\"\n",
    "            label_path.write_text(label_content)\n",
    "            \n",
    "    process_split(train_obs, \"train\")\n",
    "    process_split(val_obs, \"val\")\n",
    "    \n",
    "    # Create data.yaml\n",
    "    data_yaml = {\n",
    "        \"path\": str(output_path.absolute()),\n",
    "        \"train\": \"train/images\",\n",
    "        \"val\": \"val/images\",\n",
    "        \"nc\": len(class_names),\n",
    "        \"names\": class_names,\n",
    "    }\n",
    "    \n",
    "    yaml_path = output_path / \"data.yaml\"\n",
    "    with open(yaml_path, \"w\") as f:\n",
    "        yaml.dump(data_yaml, f, default_flow_style=False)\n",
    "        \n",
    "    print(f\"\\nDataset created at {output_path}\")\n",
    "    print(f\"data.yaml: {yaml_path}\")\n",
    "    \n",
    "    return str(yaml_path)\n",
    "\n",
    "# Create dataset\n",
    "DATA_YAML = create_yolo_dataset(\n",
    "    observations=all_observations,\n",
    "    train_ratio=CONFIG[\"train_val_split\"],\n",
    ")\n",
    "\n",
    "# Display data.yaml contents\n",
    "print(\"\\ndata.yaml contents:\")\n",
    "print(\"-\" * 30)\n",
    "with open(DATA_YAML) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 5b. Source Separation and Exploration\n\nSeparate images by data source (iNaturalist vs Zenodo) to compare quality and decide which sources to include in training.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pathlib import Path\nfrom collections import Counter, defaultdict\n\n# === Source Separation ===\n# Scan raw directories to identify images by source\n# RAW_DIR is set in the data loading cell above\n\nif not RAW_DIR.exists():\n    print(f\"Raw directory not found at {RAW_DIR}\")\n    print(\"Run the data loading cell first.\")\nelse:\n    # Count images per source and species\n    source_counts = defaultdict(lambda: defaultdict(int))\n    source_images = defaultdict(list)  # source -> list of (species, path)\n\n    for source_dir in sorted(RAW_DIR.iterdir()):\n        if not source_dir.is_dir():\n            continue\n        source_name = source_dir.name\n        for species_dir in sorted(source_dir.iterdir()):\n            if not species_dir.is_dir():\n                continue\n            species_code = species_dir.name\n            for img in species_dir.iterdir():\n                if img.suffix.lower() in (\".jpg\", \".jpeg\", \".png\", \".webp\"):\n                    source_counts[source_name][species_code] += 1\n                    source_images[source_name].append((species_code, img))\n\n    # Print comparison table\n    print(\"=\" * 70)\n    print(\"DATA SOURCE COMPARISON\")\n    print(\"=\" * 70)\n\n    all_species = sorted(set(\n        sp for counts in source_counts.values() for sp in counts\n    ))\n    sources = sorted(source_counts.keys())\n\n    header = f\"{'Species':<10}\" + \"\".join(f\"{s:<16}\" for s in sources) + f\"{'Total':<10}\"\n    print(header)\n    print(\"-\" * len(header))\n\n    for sp in all_species:\n        row = f\"{sp:<10}\"\n        total = 0\n        for src in sources:\n            count = source_counts[src].get(sp, 0)\n            row += f\"{count:<16}\"\n            total += count\n        row += f\"{total:<10}\"\n        print(row)\n\n    print(\"-\" * len(header))\n    totals_row = f\"{'TOTAL':<10}\"\n    grand = 0\n    for src in sources:\n        t = sum(source_counts[src].values())\n        totals_row += f\"{t:<16}\"\n        grand += t\n    totals_row += f\"{grand:<10}\"\n    print(totals_row)\n    print(\"=\" * 70)\n\n    print(f\"\\nKey observations:\")\n    print(f\"  iNaturalist: {sum(source_counts.get('inaturalist', {}).values())} images, \"\n          f\"research-grade, mostly live animals\")\n    print(f\"  Zenodo:      {sum(source_counts.get('zenodo', {}).values())} images, \"\n          f\"verified roadkill, European + some Australian\")\n    print(f\"  Zenodo 'OTHER' category: {source_counts.get('zenodo', {}).get('OTHER', 0)} \"\n          f\"images (unmapped European species)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Build separate YOLO datasets per source\nimport shutil, yaml, random\n\nDATASET_BASE = RAW_DIR.parent  # /content/data/training\n\ndef build_source_dataset(source_name, image_list, class_names, output_base, train_ratio=0.8):\n    \"\"\"Build a YOLO dataset from a list of (species_code, image_path) tuples.\"\"\"\n    out_dir = Path(output_base) / f\"dataset_{source_name}\"\n    for split in (\"train\", \"val\"):\n        (out_dir / split / \"images\").mkdir(parents=True, exist_ok=True)\n        (out_dir / split / \"labels\").mkdir(parents=True, exist_ok=True)\n    \n    valid = [(sp, p) for sp, p in image_list if sp in class_names]\n    random.shuffle(valid)\n    split_idx = int(len(valid) * train_ratio)\n    splits = {\"train\": valid[:split_idx], \"val\": valid[split_idx:]}\n    \n    class_to_idx_local = {name: idx for idx, name in enumerate(class_names)}\n    \n    for split_name, items in splits.items():\n        for species_code, img_path in items:\n            dst = out_dir / split_name / \"images\" / img_path.name\n            shutil.copy(img_path, dst)\n            class_id = class_to_idx_local[species_code]\n            label_path = out_dir / split_name / \"labels\" / f\"{img_path.stem}.txt\"\n            label_path.write_text(f\"{class_id} 0.5 0.5 1.0 1.0\\n\")\n    \n    data_yaml = {\n        \"path\": str(out_dir.absolute()),\n        \"train\": \"train/images\",\n        \"val\": \"val/images\",\n        \"nc\": len(class_names),\n        \"names\": class_names,\n    }\n    yaml_path = out_dir / \"data.yaml\"\n    with open(yaml_path, \"w\") as f:\n        yaml.dump(data_yaml, f, default_flow_style=False)\n    \n    print(f\"  {source_name}: {len(valid)} images -> {yaml_path}\")\n    return str(yaml_path)\n\n# Define class lists per source\nINAT_CLASSES = [CONFIG[\"species\"][i][\"code\"] for i in range(len(CONFIG[\"species\"]))]\nZENODO_CLASSES = INAT_CLASSES + [\"FOX\", \"HARE\", \"HEDGE\", \"OTHER\"]\nCOMBINED_CLASSES = sorted(set(INAT_CLASSES + ZENODO_CLASSES))\n\nprint(\"Building separate datasets per source...\")\nprint(\"=\" * 50)\n\nSOURCE_YAMLS = {}\nfor source_name, images in source_images.items():\n    if source_name == \"inaturalist\":\n        cls_list = INAT_CLASSES\n    elif source_name == \"zenodo\":\n        cls_list = [sp for sp in ZENODO_CLASSES if any(s == sp for s, _ in images)]\n    else:\n        cls_list = COMBINED_CLASSES\n    \n    yaml_path = build_source_dataset(source_name, images, cls_list, DATASET_BASE)\n    SOURCE_YAMLS[source_name] = yaml_path\n\n# Build combined\nall_images = []\nfor imgs in source_images.values():\n    all_images.extend(imgs)\ncombined_cls = sorted(set(sp for sp, _ in all_images))\nSOURCE_YAMLS[\"combined\"] = build_source_dataset(\"combined\", all_images, combined_cls, DATASET_BASE)\n\nprint(f\"\\nDataset paths:\")\nfor name, path in SOURCE_YAMLS.items():\n    print(f\"  {name}: {path}\")\n\n# === SELECT WHICH SOURCE TO USE FOR TRAINING ===\n#   \"inaturalist\" - Australian wildlife only (~1,600 images)\n#   \"zenodo\"      - European roadkill (~330 images)\n#   \"combined\"    - All sources merged\nTRAINING_SOURCE = \"inaturalist\"  # <-- Change this to select source\nDATA_YAML = SOURCE_YAMLS[TRAINING_SOURCE]\nprint(f\"\\nSelected for training: {TRAINING_SOURCE} -> {DATA_YAML}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import fiftyone as fo\nimport fiftyone.zoo as foz\nfrom pathlib import Path\n\n# Load dataset into FiftyOne\nprint(\"Loading dataset into FiftyOne...\")\n\n# Get class names\nclass_names = [CONFIG[\"species\"][i][\"code\"] for i in range(len(CONFIG[\"species\"]))]\n\n# Create FiftyOne dataset from YOLO format\ndataset_path = Path(DATA_YAML).parent\n\n# Load training split\ndataset = fo.Dataset.from_dir(\n    dataset_type=fo.types.YOLOv5Dataset,\n    dataset_dir=str(dataset_path),\n    split=\"train\",\n    name=\"roadeye-wildlife-train\",\n)\n\n# Add validation split\nval_dataset = fo.Dataset.from_dir(\n    dataset_type=fo.types.YOLOv5Dataset,\n    dataset_dir=str(dataset_path),\n    split=\"val\",\n)\n\n# Merge into main dataset with tags\nfor sample in dataset:\n    sample.tags.append(\"train\")\n    sample.save()\n\nfor sample in val_dataset:\n    sample.tags.append(\"val\")\n\ndataset.merge_samples(val_dataset)\n\n# Add source tags based on filename origin\n# Build reverse lookup: image filename -> source\nfilename_to_source = {}\nfor source_name, images in source_images.items():\n    for _, img_path in images:\n        filename_to_source[img_path.name] = source_name\n\nfor sample in dataset:\n    img_name = Path(sample.filepath).name\n    source = filename_to_source.get(img_name, \"unknown\")\n    sample.tags.append(source)\n    sample[\"source\"] = source\n    sample.save()\n\nprint(f\"\\nFiftyOne dataset loaded:\")\nprint(f\"  Total samples: {len(dataset)}\")\nprint(f\"  Train samples: {len(dataset.match_tags('train'))}\")\nprint(f\"  Val samples: {len(dataset.match_tags('val'))}\")\n\n# Source breakdown\nfor src in sorted(set(s[\"source\"] for s in dataset if s.get(\"source\"))):\n    count = len(dataset.match(fo.ViewField(\"source\") == src))\n    print(f\"  Source '{src}': {count}\")\n\nprint(dataset)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch FiftyOne App for visual exploration\n",
    "# Note: In Colab, this opens in a new tab\n",
    "\n",
    "session = fo.launch_app(dataset)\n",
    "\n",
    "print(\"\\nFiftyOne App Features:\")\n",
    "print(\"  - Filter by species using the sidebar\")\n",
    "print(\"  - Click samples to see bounding boxes\")\n",
    "print(\"  - Use the Brain to find duplicates/outliers\")\n",
    "print(\"  - Tag samples for review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics and analysis\n",
    "print(\"Dataset Statistics\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Count by species\n",
    "print(\"\\nSamples per species:\")\n",
    "for species_code in class_names:\n",
    "    view = dataset.filter_labels(\n",
    "        \"ground_truth\",\n",
    "        fo.ViewField(\"label\") == species_code\n",
    "    )\n",
    "    print(f\"  {species_code}: {len(view)}\")\n",
    "\n",
    "# Check for potential issues\n",
    "print(\"\\nData Quality Checks:\")\n",
    "\n",
    "# Check for samples without labels\n",
    "no_labels = dataset.match(fo.ViewField(\"ground_truth.detections\").length() == 0)\n",
    "print(f\"  Samples without labels: {len(no_labels)}\")\n",
    "\n",
    "# Check image dimensions\n",
    "import fiftyone.core.media as fom\n",
    "dataset.compute_metadata()\n",
    "\n",
    "# Get size distribution\n",
    "widths = [s.metadata.width for s in dataset if s.metadata]\n",
    "heights = [s.metadata.height for s in dataset if s.metadata]\n",
    "if widths:\n",
    "    print(f\"  Image width range: {min(widths)} - {max(widths)}\")\n",
    "    print(f\"  Image height range: {min(heights)} - {max(heights)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 6b. Auto-Labelling with Foundation Models\n\nReplace placeholder bounding boxes (`0.5 0.5 1.0 1.0`) with real detections using Grounding DINO or YOLO-World. Run this section on GPU.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Install auto-labelling dependencies\n!pip install -q transformers\n\n# === Auto-Labelling Configuration ===\nAUTOLABEL_CONFIG = {\n    # Method: \"grounding_dino\" or \"yolo_world\"\n    \"method\": \"grounding_dino\",\n    \n    # Grounding DINO settings\n    \"dino_model\": \"IDEA-Research/grounding-dino-base\",\n    \"box_threshold\": 0.20,\n    \"text_threshold\": 0.15,\n    \n    # YOLO-World settings (alternative)\n    \"yolo_world_model\": \"yolov8x-worldv2.pt\",\n    \"yolo_world_conf\": 0.20,\n}\n\n# Species prompts - multiple text prompts per species to improve recall\n# Foundation models know \"cat\" better than \"feral cat\", \"pigeon\" better than \"bronzewing\"\nSPECIES_PROMPTS = {\n    \"DEVIL\": [\"tasmanian devil\"],\n    \"FCAT\": [\"cat\", \"feral cat\"],\n    \"PADEM\": [\"pademelon\", \"small wallaby\"],\n    \"WALBY\": [\"wallaby\", \"kangaroo\"],\n    \"WOMBAT\": [\"wombat\"],\n    \"BPOSM\": [\"possum\", \"brushtail possum\"],\n    \"FDEER\": [\"deer\", \"fallow deer\"],\n    \"BANDI\": [\"bandicoot\", \"small mammal\"],\n    \"BRONZ\": [\"pigeon\", \"bird\", \"common bronzewing\"],\n    \"CURRA\": [\"currawong\", \"black bird\"],\n    \"FOX\": [\"fox\", \"red fox\"],\n    \"HARE\": [\"hare\", \"rabbit\"],\n    \"HEDGE\": [\"hedgehog\"],\n    \"OTHER\": [\"animal\"],\n}\n\nprint(f\"Auto-labelling method: {AUTOLABEL_CONFIG['method']}\")\nprint(f\"Box threshold: {AUTOLABEL_CONFIG['box_threshold']}\")\nprint(f\"Species with prompts: {len(SPECIES_PROMPTS)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nimport json\n\n# Read data.yaml to get class names for the selected dataset\nwith open(DATA_YAML) as f:\n    data_config = yaml.safe_load(f)\nautolabel_classes = data_config[\"names\"]\nclass_to_idx = {name: idx for idx, name in enumerate(autolabel_classes)}\n\n# Build prompt-to-class mapping\nprompt_to_code = {}\nall_prompts = []\nfor species_code in autolabel_classes:\n    prompts = SPECIES_PROMPTS.get(species_code, [species_code.lower()])\n    for p in prompts:\n        prompt_to_code[p] = species_code\n        all_prompts.append(p)\n\n# Build species map from folder structure for cross-validation\nspecies_map = {}  # filename -> expected species\nfor source_name, images in source_images.items():\n    for species_code, img_path in images:\n        species_map[img_path.name] = species_code\n\n# Output directory for auto-generated labels\nLABELS_OUTPUT = Path(DATA_YAML).parent.parent / \"auto_labels\"\nLABELS_OUTPUT.mkdir(parents=True, exist_ok=True)\n\n# Collect all images from the selected dataset\ndataset_dir = Path(DATA_YAML).parent\nimage_paths = list((dataset_dir / \"train\" / \"images\").glob(\"*.jpg\"))\nimage_paths += list((dataset_dir / \"val\" / \"images\").glob(\"*.jpg\"))\nprint(f\"Images to label: {len(image_paths)}\")\n\nif AUTOLABEL_CONFIG[\"method\"] == \"grounding_dino\":\n    from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n    \n    print(f\"Loading Grounding DINO: {AUTOLABEL_CONFIG['dino_model']}\")\n    processor = AutoProcessor.from_pretrained(AUTOLABEL_CONFIG[\"dino_model\"])\n    dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(\n        AUTOLABEL_CONFIG[\"dino_model\"]\n    ).to(\"cuda\")\n    \n    text_prompt = \". \".join(all_prompts) + \".\"\n    print(f\"Text prompt: {text_prompt[:100]}...\")\n    \n    report = {\"labelled\": 0, \"empty\": 0, \"disagreements\": [], \"per_class\": {}}\n    all_confs = []\n    \n    for img_path in tqdm(image_paths, desc=\"Auto-labelling with DINO\"):\n        image = Image.open(img_path)\n        w, h = image.size\n        \n        inputs = processor(images=image, text=text_prompt, return_tensors=\"pt\").to(\"cuda\")\n        with torch.no_grad():\n            outputs = dino_model(**inputs)\n        \n        results = processor.post_process_grounded_object_detection(\n            outputs, inputs[\"input_ids\"],\n            box_threshold=AUTOLABEL_CONFIG[\"box_threshold\"],\n            text_threshold=AUTOLABEL_CONFIG[\"text_threshold\"],\n            target_sizes=[(h, w)],\n        )\n        \n        lines = []\n        detected_species = []\n        for box, score, label_text in zip(\n            results[0][\"boxes\"], results[0][\"scores\"], results[0][\"labels\"]\n        ):\n            label_lower = label_text.lower().strip()\n            matched_code = None\n            for phrase, code in prompt_to_code.items():\n                if phrase in label_lower or label_lower in phrase:\n                    matched_code = code\n                    break\n            if matched_code is None or matched_code not in class_to_idx:\n                continue\n            \n            x1, y1, x2, y2 = box.tolist()\n            cx = ((x1 + x2) / 2) / w\n            cy = ((y1 + y2) / 2) / h\n            bw = (x2 - x1) / w\n            bh = (y2 - y1) / h\n            cid = class_to_idx[matched_code]\n            lines.append(f\"{cid} {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\")\n            all_confs.append(float(score))\n            detected_species.append(matched_code)\n            report[\"per_class\"][matched_code] = report[\"per_class\"].get(matched_code, 0) + 1\n        \n        label_path = LABELS_OUTPUT / f\"{img_path.stem}.txt\"\n        label_path.write_text(\"\\n\".join(lines))\n        \n        if lines:\n            report[\"labelled\"] += 1\n        else:\n            report[\"empty\"] += 1\n        \n        # Cross-validate against known species\n        expected = species_map.get(img_path.name)\n        if expected and detected_species and expected not in detected_species:\n            report[\"disagreements\"].append({\n                \"image\": img_path.name, \"expected\": expected, \"detected\": detected_species\n            })\n    \n    report[\"avg_confidence\"] = sum(all_confs) / len(all_confs) if all_confs else 0\n    del dino_model  # Free GPU memory\n    torch.cuda.empty_cache()\n\nelif AUTOLABEL_CONFIG[\"method\"] == \"yolo_world\":\n    from ultralytics import YOLO as YOLOWorld\n    \n    print(f\"Loading YOLO-World: {AUTOLABEL_CONFIG['yolo_world_model']}\")\n    yw_model = YOLOWorld(AUTOLABEL_CONFIG[\"yolo_world_model\"])\n    \n    # Use first prompt per species as class text\n    class_texts = [SPECIES_PROMPTS.get(c, [c.lower()])[0] for c in autolabel_classes]\n    yw_model.set_classes(class_texts)\n    \n    results = yw_model.predict(\n        source=[str(p) for p in image_paths],\n        conf=AUTOLABEL_CONFIG[\"yolo_world_conf\"],\n        device=\"cuda\",\n        verbose=False,\n    )\n    \n    report = {\"labelled\": 0, \"empty\": 0, \"disagreements\": [], \"per_class\": {}}\n    all_confs = []\n    \n    for img_path, result in zip(image_paths, results):\n        boxes = result.boxes\n        lines = []\n        detected_species = []\n        for i in range(len(boxes)):\n            box = boxes.xywhn[i].cpu().numpy()\n            cls_id = int(boxes.cls[i])\n            conf = float(boxes.conf[i])\n            if cls_id < len(autolabel_classes):\n                cx, cy, bw, bh = box\n                lines.append(f\"{cls_id} {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\")\n                all_confs.append(conf)\n                sp = autolabel_classes[cls_id]\n                detected_species.append(sp)\n                report[\"per_class\"][sp] = report[\"per_class\"].get(sp, 0) + 1\n        \n        label_path = LABELS_OUTPUT / f\"{img_path.stem}.txt\"\n        label_path.write_text(\"\\n\".join(lines))\n        \n        if lines:\n            report[\"labelled\"] += 1\n        else:\n            report[\"empty\"] += 1\n        \n        expected = species_map.get(img_path.name)\n        if expected and detected_species and expected not in detected_species:\n            report[\"disagreements\"].append({\n                \"image\": img_path.name, \"expected\": expected, \"detected\": detected_species\n            })\n    \n    report[\"avg_confidence\"] = sum(all_confs) / len(all_confs) if all_confs else 0\n    del yw_model\n    torch.cuda.empty_cache()\n\n# Save report\nreport_path = LABELS_OUTPUT / \"labelling_report.json\"\nwith open(report_path, \"w\") as f:\n    json.dump(report, f, indent=2)\n\nprint(f\"\\nAuto-labelling Results\")\nprint(f\"=\" * 40)\nprint(f\"  Labelled: {report['labelled']}\")\nprint(f\"  Empty (no detections): {report['empty']}\")\nprint(f\"  Avg confidence: {report['avg_confidence']:.3f}\")\nprint(f\"  Disagreements with folder labels: {len(report['disagreements'])}\")\nprint(f\"\\nDetections per class:\")\nfor cls, count in sorted(report[\"per_class\"].items()):\n    print(f\"  {cls}: {count}\")\nprint(f\"\\nLabels saved to: {LABELS_OUTPUT}\")\nprint(f\"Report saved to: {report_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Copy auto-labels into the dataset, replacing placeholders\nimport shutil\n\ndataset_dir = Path(DATA_YAML).parent\nreplaced = 0\n\nfor split in (\"train\", \"val\"):\n    labels_dir = dataset_dir / split / \"labels\"\n    for label_file in labels_dir.glob(\"*.txt\"):\n        auto_label = LABELS_OUTPUT / label_file.name\n        if auto_label.exists():\n            content = auto_label.read_text().strip()\n            if content:  # Only replace if auto-label has detections\n                shutil.copy(auto_label, label_file)\n                replaced += 1\n\nprint(f\"Replaced {replaced} placeholder labels with auto-generated bounding boxes\")\nprint(f\"Remaining placeholders: {len(image_paths) - replaced}\")\n\n# Verify: check a few labels are no longer placeholders\nsample_labels = list((dataset_dir / \"train\" / \"labels\").glob(\"*.txt\"))[:5]\nprint(\"\\nSample labels (should show real coordinates, not 0.5 0.5 1.0 1.0):\")\nfor lbl in sample_labels:\n    print(f\"  {lbl.name}: {lbl.read_text().strip()[:80]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Phase 1: Frozen Backbone Training with YOLO11\n",
    "\n",
    "Train with the backbone frozen to preserve pre-trained features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from ultralytics import YOLO\n\nprint(\"Phase 1: Frozen Backbone Training (YOLO11)\")\nprint(\"=\" * 50)\n\n# Load model â€” either custom NAM architecture or stock YOLO11\nif CONFIG[\"use_nam_attention\"]:\n    # Re-register NAMBlock in case this cell runs in a fresh runtime\n    import ultralytics.nn.modules as _modules\n    from ultralytics.nn import tasks as _tasks\n    if not hasattr(_modules, \"NAMBlock\"):\n        _modules.NAMBlock = NAMBlock\n    if not hasattr(_tasks, \"NAMBlock\"):\n        _tasks.NAMBlock = NAMBlock\n\n    # Write the NAM YAML config if it doesn't exist yet\n    NAM_YAML = \"/content/configs/yolo11m-nam.yaml\"\n    Path(\"/content/configs\").mkdir(parents=True, exist_ok=True)\n    if not Path(NAM_YAML).exists():\n        Path(NAM_YAML).write_text(\"\"\"\\\n# YOLO11m with NAM (Normalisation-based Attention Module)\nnc: 9\nscales:\n  m: [0.50, 1.00, 512]\n\nbackbone:\n  - [-1, 1, Conv, [64, 3, 2]]           # 0-P1/2\n  - [-1, 1, Conv, [128, 3, 2]]          # 1-P2/4\n  - [-1, 2, C3k2, [256, False, 0.25]]   # 2\n  - [-1, 1, Conv, [256, 3, 2]]          # 3-P3/8\n  - [-1, 2, C3k2, [512, False, 0.25]]   # 4\n  - [-1, 1, NAMBlock, [512]]            # 5 - NAM after P3 features\n  - [-1, 1, Conv, [512, 3, 2]]          # 6-P4/16\n  - [-1, 2, C3k2, [512, True]]          # 7\n  - [-1, 1, NAMBlock, [512]]            # 8 - NAM after P4 features\n  - [-1, 1, Conv, [1024, 3, 2]]         # 9-P5/32\n  - [-1, 2, C3k2, [1024, True]]         # 10\n  - [-1, 1, SPPF, [1024, 5]]            # 11\n  - [-1, 2, C2PSA, [1024]]              # 12\n\nhead:\n  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]  # 13\n  - [[-1, 8], 1, Concat, [1]]           # 14 - cat backbone P4 (after NAM)\n  - [-1, 2, C3k2, [512, False]]         # 15\n  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]  # 16\n  - [[-1, 5], 1, Concat, [1]]           # 17 - cat backbone P3 (after NAM)\n  - [-1, 2, C3k2, [256, False]]         # 18 (P3/8-small)\n  - [-1, 1, Conv, [256, 3, 2]]          # 19\n  - [[-1, 15], 1, Concat, [1]]          # 20 - cat head P4\n  - [-1, 2, C3k2, [512, False]]         # 21 (P4/16-medium)\n  - [-1, 1, Conv, [512, 3, 2]]          # 22\n  - [[-1, 12], 1, Concat, [1]]          # 23 - cat head P5\n  - [-1, 2, C3k2, [1024, True]]         # 24 (P5/32-large)\n  - [[18, 21, 24], 1, Detect, [nc]]     # 25 - Detect(P3, P4, P5)\n\"\"\")\n        print(f\"  Created NAM YAML at {NAM_YAML}\")\n\n    print(f\"Architecture: {NAM_YAML} (YOLO11m + NAM attention)\")\n    print(f\"Pretrained weights: {CONFIG['base_model']}\")\n    model = YOLO(NAM_YAML)\n    model.load(CONFIG['base_model'])\n\n    # NAM inserts 2 extra layers in the backbone (layers 5, 8),\n    # so freeze 12 to cover the full backbone instead of 10\n    freeze_layers = 12\n    print(f\"Freeze layers: {freeze_layers} (adjusted for NAM backbone)\")\nelse:\n    print(f\"Architecture: stock {CONFIG['base_model']}\")\n    model = YOLO(CONFIG['base_model'])\n    freeze_layers = CONFIG[\"freeze_layers\"]\n    print(f\"Freeze layers: {freeze_layers}\")\n\nprint(f\"Epochs: {CONFIG['epochs_phase1']}\")\nprint(f\"Learning rate: 0.001\")\nprint()\n\n# Train with frozen backbone\nresults_phase1 = model.train(\n    data=DATA_YAML,\n    epochs=CONFIG[\"epochs_phase1\"],\n    batch=CONFIG[\"batch_size\"],\n    imgsz=CONFIG[\"image_size\"],\n    patience=CONFIG[\"patience\"],\n    project=CONFIG[\"project_name\"],\n    name=\"phase1_frozen_yolo11\",\n    exist_ok=True,\n\n    freeze=freeze_layers,\n    lr0=0.001,\n    lrf=0.01,\n\n    hsv_h=0.015,\n    hsv_s=0.7,\n    hsv_v=0.4,\n    degrees=0.0,\n    translate=0.1,\n    scale=0.5,\n    fliplr=0.5,\n    mosaic=1.0,\n\n    device=0,\n    workers=4,\n)\n\nprint(\"\\nPhase 1 training complete!\")\nPHASE1_MODEL = f\"{CONFIG['project_name']}/phase1_frozen_yolo11/weights/best.pt\"\nprint(f\"Best model: {PHASE1_MODEL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Phase 2: Fine-tuning (Optional)\n",
    "\n",
    "Unfreeze all layers and fine-tune with very low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "RUN_PHASE2 = True  # Set to False to skip fine-tuning\n\nif RUN_PHASE2:\n    print(\"Phase 2: Fine-tuning (All Layers)\")\n    print(\"=\" * 50)\n    print(f\"Starting from: {PHASE1_MODEL}\")\n    print(f\"Epochs: {CONFIG['epochs_phase2']}\")\n    print(f\"Learning rate: 0.0001 (very low to preserve features)\")\n    print()\n\n    # Re-register NAMBlock if using NAM (needed if runtime restarted)\n    if CONFIG[\"use_nam_attention\"]:\n        import ultralytics.nn.modules as _modules\n        from ultralytics.nn import tasks as _tasks\n        if not hasattr(_modules, \"NAMBlock\"):\n            _modules.NAMBlock = NAMBlock\n        if not hasattr(_tasks, \"NAMBlock\"):\n            _tasks.NAMBlock = NAMBlock\n        print(\"NAMBlock registered (Phase 2)\")\n\n    # Load Phase 1 model (already has NAM architecture if enabled)\n    model_phase2 = YOLO(PHASE1_MODEL)\n\n    # Fine-tune with all layers unfrozen\n    results_phase2 = model_phase2.train(\n        data=DATA_YAML,\n        epochs=CONFIG[\"epochs_phase2\"],\n        batch=CONFIG[\"batch_size\"],\n        imgsz=CONFIG[\"image_size\"],\n        patience=CONFIG[\"patience\"],\n        project=CONFIG[\"project_name\"],\n        name=\"phase2_finetune_yolo11\",\n        exist_ok=True,\n\n        # No freezing - all layers trainable\n        freeze=0,\n\n        # Very low learning rate to preserve knowledge\n        lr0=0.0001,\n        lrf=0.001,\n\n        # Same augmentation\n        hsv_h=0.015,\n        hsv_s=0.7,\n        hsv_v=0.4,\n        degrees=0.0,\n        translate=0.1,\n        scale=0.5,\n        fliplr=0.5,\n        mosaic=1.0,\n\n        device=0,\n        workers=4,\n    )\n\n    FINAL_MODEL = f\"{CONFIG['project_name']}/phase2_finetune_yolo11/weights/best.pt\"\n    print(\"\\nPhase 2 fine-tuning complete!\")\nelse:\n    FINAL_MODEL = PHASE1_MODEL\n    print(\"Skipping Phase 2 - using Phase 1 model\")\n\nprint(f\"Final model: {FINAL_MODEL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation with FiftyOne\n",
    "\n",
    "Use FiftyOne to visualise model predictions and identify failure modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final model\n",
    "print(\"Evaluating Final Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model_eval = YOLO(FINAL_MODEL)\n",
    "metrics = model_eval.val(data=DATA_YAML)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  mAP50: {metrics.box.map50:.4f}\")\n",
    "print(f\"  mAP50-95: {metrics.box.map:.4f}\")\n",
    "print(f\"  Precision: {metrics.box.mp:.4f}\")\n",
    "print(f\"  Recall: {metrics.box.mr:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(f\"\\nPer-class mAP50:\")\n",
    "class_names = [CONFIG[\"species\"][i][\"code\"] for i in range(len(CONFIG[\"species\"]))]\n",
    "for i, name in enumerate(class_names):\n",
    "    if i < len(metrics.box.ap50):\n",
    "        print(f\"  {name}: {metrics.box.ap50[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model predictions to FiftyOne dataset for analysis\n",
    "print(\"Adding model predictions to FiftyOne...\")\n",
    "\n",
    "# Get validation samples\n",
    "val_view = dataset.match_tags(\"val\")\n",
    "\n",
    "# Run inference on validation set\n",
    "for sample in tqdm(val_view, desc=\"Running inference\"):\n",
    "    # Run YOLO inference\n",
    "    results = model_eval(sample.filepath, verbose=False)\n",
    "    \n",
    "    # Convert predictions to FiftyOne format\n",
    "    detections = []\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for i in range(len(boxes)):\n",
    "            # Get box in YOLO format (x_center, y_center, width, height) normalised\n",
    "            box = boxes.xywhn[i].cpu().numpy()\n",
    "            x, y, w, h = box\n",
    "            \n",
    "            # Convert to FiftyOne format (x, y, width, height) - top-left corner\n",
    "            fo_box = [x - w/2, y - h/2, w, h]\n",
    "            \n",
    "            cls_id = int(boxes.cls[i])\n",
    "            conf = float(boxes.conf[i])\n",
    "            label = class_names[cls_id] if cls_id < len(class_names) else \"unknown\"\n",
    "            \n",
    "            detections.append(\n",
    "                fo.Detection(\n",
    "                    label=label,\n",
    "                    bounding_box=fo_box,\n",
    "                    confidence=conf,\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # Add predictions to sample\n",
    "    sample[\"predictions\"] = fo.Detections(detections=detections)\n",
    "    sample.save()\n",
    "\n",
    "print(\"\\nPredictions added. Launching FiftyOne for model evaluation...\")\n",
    "\n",
    "# Evaluate predictions against ground truth\n",
    "results = val_view.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    eval_key=\"eval\",\n",
    "    compute_mAP=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nFiftyOne Evaluation Results:\")\n",
    "print(results.report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch FiftyOne to explore predictions and failures\n",
    "session = fo.launch_app(val_view)\n",
    "\n",
    "print(\"\\nFiftyOne Model Evaluation Features:\")\n",
    "print(\"  - Compare predictions vs ground truth\")\n",
    "print(\"  - Filter by TP/FP/FN using eval field\")\n",
    "print(\"  - Sort by confidence to find threshold\")\n",
    "print(\"  - Identify failure modes by species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export and Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Exporting Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate timestamp for versioning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create output directory\n",
    "export_dir = f\"{DRIVE_OUTPUT}/models/{timestamp}\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Copy PyTorch model\n",
    "pt_dest = f\"{export_dir}/roadeye_yolo11_{timestamp}.pt\"\n",
    "shutil.copy(FINAL_MODEL, pt_dest)\n",
    "print(f\"PyTorch model saved: {pt_dest}\")\n",
    "\n",
    "# Export to ONNX (for deployment)\n",
    "model_export = YOLO(FINAL_MODEL)\n",
    "onnx_path = model_export.export(format=\"onnx\")\n",
    "onnx_dest = f\"{export_dir}/roadeye_yolo11_{timestamp}.onnx\"\n",
    "shutil.copy(onnx_path, onnx_dest)\n",
    "print(f\"ONNX model saved: {onnx_dest}\")\n",
    "\n",
    "# Save training config\n",
    "config_path = f\"{export_dir}/training_config.yaml\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(CONFIG, f, default_flow_style=False)\n",
    "print(f\"Config saved: {config_path}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_data = {\n",
    "    \"map50\": float(metrics.box.map50),\n",
    "    \"map50_95\": float(metrics.box.map),\n",
    "    \"precision\": float(metrics.box.mp),\n",
    "    \"recall\": float(metrics.box.mr),\n",
    "    \"model\": \"YOLO11\",\n",
    "    \"timestamp\": timestamp,\n",
    "}\n",
    "metrics_path = f\"{export_dir}/metrics.yaml\"\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    yaml.dump(metrics_data, f, default_flow_style=False)\n",
    "print(f\"Metrics saved: {metrics_path}\")\n",
    "\n",
    "print(f\"\\nAll exports saved to: {export_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Training complete with **YOLO11** and **NAM attention** module.\n",
    "\n",
    "**Key improvements over YOLOv8:**\n",
    "- 22% fewer parameters\n",
    "- Better small object detection (important for roadside wildlife)\n",
    "- NAM attention enhances feature extraction\n",
    "\n",
    "**Next steps:**\n",
    "1. Download the `.pt` file from Google Drive\n",
    "2. Use FiftyOne to review failure cases and improve labels\n",
    "3. Consider manual annotation with CVAT/LabelStudio for better bounding boxes\n",
    "\n",
    "**Labelling Tools (for manual annotation):**\n",
    "- **CVAT** - Free, web-based, industry standard: https://cvat.ai\n",
    "- **LabelStudio** - Free, self-hosted option: https://labelstud.io\n",
    "- **LabelImg** - Simple desktop app for Mac: `pip install labelImg`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}